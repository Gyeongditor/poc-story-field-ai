# import torch
# from kospeech.models.deepspeech2 import DeepSpeech2
# from kospeech.vocabs.ksponspeech import KsponSpeechVocabulary
# from kospeech.data.audio.processor import SpectrogramParser
# from kospeech.opt import load_config

# # ê²½ë¡œ ì„¤ì •
# model_path = 'deepspeech2.pt'  # ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”
# config_path = 'config.yaml'
# audio_path = '/mnt/data/90457_ì—¬í–‰.wav'

# # ë””ë°”ì´ìŠ¤ ì„¤ì •
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# # config ë° vocab ë¡œë“œ
# config = load_config(config_path)
# vocab = KsponSpeechVocabulary()
# parser = SpectrogramParser(audio_config=config['audio'], vocab=vocab, feature_extract_by='torchaudio')

# # ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ
# features = parser.parse(audio_path).unsqueeze(0).to(device)

# # ëª¨ë¸ ë¡œë“œ
# model = DeepSpeech2(vocab, config['model'])
# checkpoint = torch.load(model_path, map_location=device)
# model.load_state_dict(checkpoint['model_state_dict'])
# model.eval().to(device)

# # ì˜ˆì¸¡
# with torch.no_grad():
#     output = model(features)
#     decoded = vocab.label_to_string(output.argmax(dim=-1))

# print("ğŸ—£ï¸ ì¸ì‹ëœ í…ìŠ¤íŠ¸:", decoded[0])
import torch
print(torch.__version__)