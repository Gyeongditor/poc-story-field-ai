{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8578dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\POC_STT\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d605ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182653cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c242271",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_model ='skt/kogpt2-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c32999",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(token_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(token_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61ddd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fdbe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#프롬포트 기반 LLM 생성기(skt/kogpt2용)\n",
    "def generate_kogpt(prompt: str, max_length=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    input_ids = inputs['input_ids']\n",
    "    print(f\"[DEBUG] Prompt token length: {input_ids.shape[1]}\")\n",
    "    print(f\"[DEBUG] Max token id: {input_ids.max().item()}, Vocab size: {tokenizer.vocab_size}\")\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.8)\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    generated_tokens = outputs[0][prompt_len:]\n",
    "    generated_only = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    return generated_only\n",
    "\n",
    "\n",
    "#프롬포트 기반 LLM 생성기(EleutherAI/polyglot-ko-1.3B)\n",
    "# def generate_kogpt(prompt):\n",
    "#     inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "#     # ✅ token_type_ids 제거\n",
    "#     if 'token_type_ids' in inputs:\n",
    "#         del inputs['token_type_ids']\n",
    "    \n",
    "#     print(f\"[DEBUG] Prompt token length: {inputs['input_ids'].shape[1]}\")\n",
    "    \n",
    "#     inputs = inputs.to(device)\n",
    "#     outputs = model.generate(\n",
    "#         **inputs,\n",
    "#         max_new_tokens=256,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.8\n",
    "#     )\n",
    "#     generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     generated_only = generated[len(prompt):].strip()\n",
    "#     return generated_only\n",
    "\n",
    "\n",
    "# #디버깅용\n",
    "# def generate_kogpt(prompt: str, max_length=512):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "#     input_ids = inputs['input_ids'][0]\n",
    "#     print(f\"Input max token id: {input_ids.max()}, vocab size: {tokenizer.vocab_size}\")\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.8)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb680736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순한 토큰 단위 분할\n",
    "def chunk_text(text, max_chars=600):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), max_chars):\n",
    "        chunks.append(text[i:i+max_chars])\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71be304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_written(spoken_text):\n",
    "    prompt = f\"\"\"\n",
    "아래 구술체를 자연스럽고 정중한 문어체로 변환해 주세요.\n",
    "\n",
    "예시)\n",
    "구술체: \"오늘 날씨 진짜 좋았어.\"\n",
    "문어체: \"오늘 날씨가 매우 좋았습니다.\"\n",
    "\n",
    "[변환할 텍스트]\n",
    "{spoken_text}\n",
    "\"\"\"\n",
    "    return generate_kogpt(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "174dca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#요약\n",
    "def summarize(text):\n",
    "    prompt = f\"\"\"다음 내용을 요약해 주세요 (중요한 장소, 인물, 경험 중심으로 간결하게):\\n{text}\\n\\n요약:\"\"\"\n",
    "    return generate_kogpt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5002df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#동화 생성\n",
    "def generate_fairytale(summary):\n",
    "    prompt = f\"\"\"다음 내용을 바탕으로 초등학생용 창의적인 동화를 만들어주세요. 제목도 포함해주세요.\\n\\n내용:\\n{summary}\\n\\n동화:\"\"\"\n",
    "    return generate_kogpt(prompt, max_length=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46be9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여행, 여행하면 혹시 기억에 남는 여행이라든지 아니면 엄청 좋았던 여행에 대해서 말씀해 주실까 있을까요? 네. 여행하면 배를 타고, 인차를 세 배를 타고 중국 단동으로 가서 그 에이에 백두산. 음. 백두산 간 적이\n",
      " 있는데요. 네. 우리는 이제 문화원에서 가는 거예요. 그래서 약 한 120명 정도? 음. 많이가 좋네요. 그런데 그 문화원에 120명 정도 백두산만 간 게 아니라 러시아를 10박 11일, 인도 10박 11일, 뭐 중국, 뭐 할 것  \n",
      "없이 이번에는 그 울릉도 갔다 왔고요. 그래서 늘 그 단체로 가기 때문에 문화원, 우리 왓프의 그 문화원에서 가기 때문에 굉장히 그 숙소 뿐만 아니라 여행지, 아니면 러시아를 갔다는 러시아에서 그 해산사를 딱 투 \n",
      "어요. 러시아 대학의, 러시아 대학원의 한국 학생이 러시아 공부를 하는 사람을 해설하는 거예요. 음. 그 너무 재밌고, 볼만하고. 거기서 어떤 거냐면 우리가 그 시베리아 그 뭐죠? 그 열차 있죠? 횡단 열차. 횡단 열 \n",
      "차에 타고, 여름이 어디에서 이렇게 축하해서 어디까지 딱 가요. 거기 내려가지고, 하루 먹고 시컨 거기서 이제 놀고 또 다시 어디를 이동한다면 거기서 다시 또 시베리아에 타고 또 어디까지 또 가는 거예요. 음. 거 \n",
      "기서 그 시빡 시비를 몽골까지 갔거든요. 음. 그래서 거기 가던 그 여행, 여행지. 그러니까 굉장히 그 시베리아 열차, 횡단 열차가 굉장히 거기 열악해요. 음. 시단시도 열악하고. 음. 거기 2층 침대, 거기 2층 침대를\n",
      " 막 4명이서 거기 올라가서 굉장히 불편하지만. 음. 그리고 굉장히 시끄럽고. 음. 그래서 밖을 보면 그냥 막막 되게 깜깜하잖아요. 밤에만 이동하니까. 그렇죠. 어디가 어딘지 잘 몰라요. 음. 그래서 아침에 내려서 다\n",
      "시 이동해서. 음. 그 무거운 친구가 이렇게 막 내려서 어저어로 가서 또 하고 또 후대로 가서 진동하고 또 다시 여행 가면 거기 이제 그 회사자가 고고마다 다르잖아요. 네. 그 회사에 드는 곳도 되게 좋았고. 음. 그 \n",
      "여행은 이제 또 다 같은 문화원 이사들 뭐 그런 사람이기 때문에 다 아는 사람들에서 편하고 좋고. 음. 그리고 이제 문화원에서 가는 여행은 되게 잠자리, 먹을거리. 네. 뭐 볼거리가 되게 짱잉쎄 있게 가요. 음. 그래\n",
      "서 늘 좋아요. 그래서 이 내년에도 봄에 일본 갈 거예요. 일본. 일본. 일본 갈 거고. 그 다음에 또 백도산을 또 간다고 그랬거든요. 백도산이 우리가 여러 코스로 가기 때문에 코스가 좀 달라요. 네. 근데 내년 봄에 4월 달에는 일본을 가고. 음. 뭐 8월 달에는. 아니. 백도산을 갈 거고. 음. 그리고 이제 또 어디를 가냐면 비행기 열착 한. 자서시 반의 비행기를 타가. 아니. 아니. 열차를 타고. 예를 들면 안동을 가거나 아니면 부산\n",
      "을 가거나 이렇게 일래를 한 번씩 꼭 기차 여행 있는데 그 기차 여행을 우리가 7명을 다 한꺼번에 대들어 얻어요. 500명, 600명이 같이 가는 거예요. 음. 그래서 칸, 칸마다 이 칸은 노는 칸. 이 칸은 편히 가고 음악 \n",
      "수를 들을 수 있게 가는 칸. 음. 춤추는 칸. 음. 이 칸은 잘 먹는 칸. 이 칸은 가게끼리 가는 칸. 너무 너무 멀쩡하면 할 말이 많아요. 음. 진짜요. 네. 그래서 이제 그렇게 가격이 만약에 싸다 그런 건 아니지만. 준 \n",
      "만큼 편안하게 잘 먹고 잘 놀다 잘 오는 것이에요. 음. 그래서 너무 좋은 기억이 들 있어요. 음. 뭐 중급도 가고 예를 들면 뭐 태국도 가고. 음. 뭐 일부도 하고 뭐 이렇게 투구도 가고. 음. 그러니까 다 그렇게 많이 \n",
      "다녔어요. 여행을 되게 자주 다니시는 편이시네요. 그렇죠. 지금 여기도 지금 어디냐면 여기 소리를 치해왔어요. 지금. 아 뭐야. 지금 막 차 세워놓고 지금 다 세우고. 아 진짜요. 지금 방 사례기후를 들어왔어요. 아. 저희 뷰테드 혹시 선생님 전화 와 가지고 통화하는 거예요. 아. 그래서 우리가. 우리가 그 주민자취에 있다라는 동산에서 주민자취에서 주민자취 회원들이 온 거예요. 음. 음. 그래서. 별로가 없었는데 제가 방해가  \n",
      "되는 건 아니에요. 그런데. 그런데 이것도 여행의 일종이고 선생님하고 하는 것도 아까 여행하니까 생각이 나서서 이제 다 충분히 맞췄면 되는 거고요. 네. 그래서. 자. 자함을 내서. 아. 아. 감사합니다. 엄청 가볍게\n",
      " 논문도 써야 되고 먹든 해야 되고. 아. 그런데. 응. 그리고 하고. 네. 네. 네. 네. 컴퓨터 들고 왔습니다. 그럼. 잠깐. 네. 그. 쉬는 타임이라고 생각하시는 좋을 것 같아요. 그렇죠. 아. 응. 너무 늦어서. 너무 바쁘\n",
      "시지만 잠깐. 네.\n"
     ]
    }
   ],
   "source": [
    "text_file = f'D:/gap_year/poc-story-field-ai/STORY/LLM/travel_test.txt'\n",
    "with open(text_file, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff6a4951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Prompt token length: 304\n",
      "[DEBUG] Max token id: 50082, Vocab size: 51200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\POC_STT\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Prompt token length: 343\n",
      "[DEBUG] Max token id: 50082, Vocab size: 51200\n",
      "[DEBUG] Prompt token length: 332\n",
      "[DEBUG] Max token id: 50082, Vocab size: 51200\n",
      "[DEBUG] Prompt token length: 263\n",
      "[DEBUG] Max token id: 51158, Vocab size: 51200\n",
      "그때 그 문화원에서 러시아 가자고 한 거예요. 저는 러시아 대학원에 갔습니다. 러시아 대학교에 갔지만. 우리 왓프의 그 호텔에서 그 호텔을 간다고 해서 그 호텔에 가서 하룻밤 묵고. 그런데 그 시간에 그 러시아 호텔에서 제가 러시아 문화원에서 갔습니다 해서 우리는 그 호텔에서 하룻밤 묵고 가서 이제 막 그 러시아 문화원에서 러시아 대학원이기 때문에 거기서 또 하룻밤에 잤습니다. 그 호텔에서 그 러시아 문화원 있는 줄 알았어요? 제가 아까 그 문화원에서 그 러시아 대학교에 갔는데. 그 러시아 대학교는 한국 대학원이기도 하고. 그 러시아 대학교에서 한국 대학원이기 때문에 그 러시아 대학교에도 한국 대학원이 있었습니다. 그 교수님이 바로 그 교수님이었어요. 그 러시아 대학교는 한국 대학원이기도 해서 그 교수님이 이제 러시아에서 하는 그 강의 들으면서 그런 강의를 하면서 그 교수님 강의를 듣고 또 그 교수님이 한국 대학원에 계셨는데. 그 한국 대학교에 가서 그 교수님 강의를 들으면서 한국 대학에서 그런 강의 들으면서 한국 대학원에 가는데 그 교수님이 강의 들으면서 그 교수님 강의를 듣고 또 교수가 한국 대학에서 강의 들으면서 교수님 강연 들으면서 한국 대학에서 강의 듣고 하는 그런 강의를 들으면서 한국 대학에서 강의 들으면서 한국 대학에서 강의 들으면서 교수님\n",
      "\n",
      "그렇게 이제 가고 있는데. 이제 우리가 가는 곳은 가기로 하고 왔어요. 그래가지고. 어.\n",
      "너무 좋았어. 진짜 너무 좋았어. 음. 그리고 또 다른 친구들. 음. 그리고 또 롯데가랑도 가고 싶고. 음. 그러니까 또 다른 친구들. 음. 그리고 또 또 그 친구들. 그리고 또 또 다른 친구들. 또 다른 친구들. 롯데가랑도 가고 싶고. 또 롯데가랑도 가고 싶고. 음. 근데 또 다른 친구들. 또 다른 친구들. 이런 친구들. 그러니까 또 다른 친구들. 그리고 또 다른 친구들. 그리고 또 다른 친구들. 그 친구들. 또 다른 친구들. 또 다른 친구들. 그 친구들. 또 다른 친구들. 음. 그래서 이제 롯데가랑도 가고 싶은. 그리고 또 다른 친구들. 또 다른 친구들. 음. 이렇게 다양한 친구들. 또 다른 친구들. 음. 또 다른 친구들. 또 다른 친구들. 또 다른 친구들. 음. 그런데 또 다른 친구들. 또 다른 친구들. 또 다른 친구들. 또 다른 친구들. 또 다른 친구들. 또 다른 친구들. 저는 이런\n",
      "\n",
      "이곳에도 소리가 있잖아요?.\n",
      "그래서 제가 여행 가면 그 소리 들을 수 있는 거야. 그래서 저는 아~ 아~ 소리가 괜찮은 거라고 생각을 했어요. 음. 그리고 그게 또 되게 좋더라고. 그게 또 되게 행복해요. 음. 이런 식으로. 아~ 잘 먹고 잘 놀다가 네. 그게 가장 좋은 게 아닌가요? 신작 웹툰으로 제작되고 있는 웹툰 미야자키 하야오 원작, 이 작품은 2009년 5월, 한국 문화콘텐츠진흥원이 선정한 웹툰에 선정되었다.\n",
      "작품명은 #미야자키 하야오 원작으로 미야자키 하야오가 직접 작사를 맡은 작품으로, 소설 속 등장인물들의 성격과 캐릭터가 어떻게 구현되는지를 잘 그려내 눈길을 끈다.\n",
      "또한, 스토리 텔링을 잘 살려서 많은 사랑을 받고 있는 '치쿠마 히라타', '히라타 야스히로' 등과 함께 이번 작품으로 선정되어 한국 웹툰계의 새로운 역사를 써나갈 것으로 보인다.\n",
      "아울러, 이번 웹툰은 원작 미야자키 하야오 원작과 더불어 일본 웹툰시장에 독보적인 작품성을 지닌 웹툰 미야자키 하야오 작품을 동시에 소개할 수 있게 되었다.\n",
      "아울러 미야자키 하야오 원작, 이번에 미야자키 하야\n",
      "\n",
      "네. 이렇게 좀 쉬는 시간도 또 필요할 것 같아요. 서울시는 19일부터 ‘서울시 택시 기본요금’을 개편한다고 밝혔다.\n",
      "서울시는 기존 기본요금은 6000∼6400원인데 택시 기본요금 700원을 800원으로 600원을 인상하는 것이다.\n",
      "서울시는 “지난해 12월 택시 기본요금 개편안 발표 이후 서울 택시업계가 요금 인상 요구를 강력히 제기했고 시민단체, 택시연맹, 택시단체 등이 합의해 요금 인상 시기를 조율하기로 했다”고 설명했다.\n",
      "서울시는 기본요금 700원이 인상되면 현행 8000원에서 9500원으로 100원 인상된다.\n",
      "서울시는 내년 1월 택시사업자들의 의견을 듣고 시의회 의견 청취와 요금 인상을 위한 시민 공청회를 열 계획이다.\n",
      "한편 서울시는 지난해 11월 택시 기본요율을 1만5000원에서 1만4000원으로 4000원 인상하는 안을 의결한 바 있다. 한낮의 최고기온이 30도를 웃도는 무더운 날씨지만, 여름처럼 뜨겁지도 않은데요.\n",
      "오늘은 장마전선의 영향으로 전국에 장맛비가 내릴 전망입니다.\n",
      "기상청은 오늘 오전까지 전국에 장맛비가 내릴 것으로 내다봤습니다.\n",
      "서울을 비롯한 전국 대부분 지역에는 오후 한때 비가 오겠습니다.\n",
      "밤사이에 돌풍과 벼락을 동반한 국지성\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_text(content, max_chars=600)  # 문자 기준 chunking\n",
    "\n",
    "written_parts = [convert_to_written(chunk) for chunk in chunks]\n",
    "written = '\\n\\n'.join(written_parts)\n",
    "print(written)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "POC_STT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
